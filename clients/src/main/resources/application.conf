//akka {
//  loggers = ["akka.event.slf4j.Slf4jLogger"]
//  loglevel = "INFO"
//  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
//  log-dead-letters = 0
//}

common {
  numMessage = "20000" #默认发送的消息数
  scheduler.time = "20" #默认的调度时间
  mode = "readwrite" #模式，分为write，read和readwrite三种模式
  threadNum = "10" #提供的资源池的线程数，主要防止kafkaconsumer造成的阻塞
  actor = "10"
  timeout = "100"
}

consumer {
  bootstrap.servers = "localhost:9092"
  group.id = "actor-kafka"
  zookeeper.connect = "localhost:2181"
  host = "localhost"
  port = "2181"
  bufferSize = "100"
  clientId = "typesafe"
  topic = "testctao"
  zookeeper.sync.time.ms = "200"
  auto.commit.interval.ms = "1000"
  zookeeper.session.timeout.ms = "5000"
  zookeeper.connection.timeout.ms = "10000"
  rebalance.backoff.ms = "2000"
  rebalance.max.retries = "10"
  key.deserializer = "org.apache.kafka.common.serialization.StringDeserializer"
  value.deserializer = "org.apache.kafka.common.serialization.LongDeserializer"
}

producer {
  metadata.broker.list = "localhost:9092"
  key.serializer = "org.apache.kafka.common.serialization.StringSerializer"
  value.serializer = "org.apache.kafka.common.serialization.LongSerializer"
  bootstrap.servers = "localhost:9092"
}